---
title: "Performing PCA on game rating data"
output:
  html_notebook: default
  html_document: default
---

This notebook covers the cleaning and subsequent PCA analysis of the elite user ratings data.  

**Credit to Matthew Borthwick for scraping and providing this data.**  
**Data Source:  boardgamegeek.com**


Our tasks are as follows:

* Reshape the data such that it is in a format that PCA likes
* Impute values for the NA's
* Perform PCA
* Perform K-means clustering using the principal components

Lets first download the data and take a look.

```{r, warning = FALSE, message = FALSE, results = 'asis'}

source("requirements.R")
ggthemr("dust")

elite <- read_csv("Data/boardgame-elite-users.csv") %>% rename(userID = `Compiled from boardgamegeek.com by Matt Borthwick`)

titles <- read_csv("Data/boardgame-titles.csv") %>% rename(gameID = `boardgamegeek.com game ID`)

elite[1:5,] %>% kable("html") %>%
                kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))

titles[1:5,] %>% kable("html") %>%
                 kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))

```

<br>

We can explore the data a bit more.  Lets calculate the average and standard deviation of ratings for each game as well as how many times it was reviewed.  A plot of average rating shows somewhat of an upward trend as number of reviews increases.

<br>

```{r, warning = FALSE, message = FALSE}
#Group by game, calculate the within gruoup average, standard deviation, and ratings count, and join with the titles dataframe.
elite <- elite %>% group_by(gameID) %>% 
              mutate(avg_rating = mean(rating), sd_rating = sd(rating), freq = n()) %>%  
              left_join(titles) 

#kable() function makes nice tables
elite[1:5,] %>% kable("html") %>%
                kable_styling(full_width = FALSE, position = "left", 
                              bootstrap_options = c("striped", "hover", "responsive"))

#Plot mean and standard deviation against review frequency
ggplot(elite, aes(x = x)) + 
  geom_point(aes(x = freq, y = sd_rating, col = "red")) + 
  geom_point(aes(x = freq, y = avg_rating, col = "blue")) +
  xlab("Number of Reviews") + ylab("Value") +
  scale_color_manual(values = c("red" = "red", "blue" = "blue"), 
                     labels = c("Average", "Standard Deviation"))
```

 

<br>

##**Data Preparation**

We ultimately want our data to contain a **single row for each user** where the columns for that user represent their rating on **each game among all games**

To get the data in this format, we lean on dplyr, and the spread() function in particular, which will convert the data into a wide format.  The below dplyr pipeline performs the following tasks from top to bottom:


```{r, warning = FALSE, message = FALSE}
elite_sparse <- elite%>%
                select(userID, gameID, rating) %>%
                spread(gameID, rating)

elite_sparse[1:5, 1:15] %>% kable("html") %>% 
                            kable_styling(full_width = FALSE, position = "left", 
                                          bootstrap_options = c("striped", "hover", "responsive"))

```

As you can see, each reviewer does not have a score for every game, and we end up with a lot of missing values even for this set of "elite" reviewers.  There are a few methods that can be used to fill the missing cells, including using the mean/median rating.  Here I will use a set of functions from the missMDA package that uses iterative methods to find likely values.  Some of the resulting data will not make sense (negative ratings, ratings higher than the maximum), however this will still provide a workable base for performing PCA.

Two steps are performed to fill the table:
1. Estimate the number of dimensions(components) to be used in PCA
2. Fill the missing cells assuming we have a number of dimensions estimated in 1.
 
```{r, warning = FALSE, message = FALSE}
ncomp <- estim_ncpPCA(as.data.frame(elite_sparse[,2:ncol(elite_sparse)]), ncp.min = 0, ncp.max = 6)

elite_sparse_filled <- imputePCA(as.data.frame(elite_sparse[,2:ncol(elite_sparse)]), 
                                 ncp = ncomp$ncp, scale = TRUE,
                                 method = "EM", row.w = NULL, coeff.ridge = 1,
                                 threshold = 1e-06, seed = NULL, nb.init = 1, maxiter = 1000)$completeObs

PCA <- prcomp(elite_sparse_filled, rank. = ncomp$ncp, retx = TRUE)

components <- as.data.frame(PCA$rotation) %>% mutate(gameID = as.numeric(rownames(.))) %>% left_join(titles) 


```


We graph the data in terms of pairs of principal components.  We can see how PCA chooses the component axes.  The data varies most in the direction of principal component 1, then principal component 2, and so on.

<br>

```{r, warning = FALSE, message = FALSE, echo = FALSE}
theme <- list(geom_point(color = "black", fill = "green", pch = 21),
              theme(plot.margin=unit(c(0.25,0.25,0.25,0.25), "cm"),
                    axis.title.x = element_text(size = 8, vjust = -0.5),
                    axis.title.y = element_text(size = 8)
                    ))

grid.arrange(
ggplot(as.data.frame(PCA$x), aes(x = PC1, y=PC2)) + theme,
ggplot(as.data.frame(PCA$x), aes(x = PC2, y=PC3)) + theme,
ggplot(as.data.frame(PCA$x), aes(x = PC3, y=PC4)) + theme,
ggplot(as.data.frame(PCA$x), aes(x = PC4, y=PC5)) + theme,
ggplot(as.data.frame(PCA$x), aes(x = PC5, y=PC6)) + theme,
ncol = 3
)
```

<br>

We can look at the top 10 loadings for each principal component and see what kinds of games are included to see if they describe some sort of genre or series of games.

```{r, warning = FALSE, message = FALSE}

cbind(top_n(components, 10, PC1) %>% desc(PC1) %>% select(PC1, title),
      top_n(components, 5, PC2) %>% select(PC2, title),
      top_n(components, 5, PC3) %>% select(PC3, title)) %>% kable("html") %>% 
                               kable_styling(full_width = FALSE, position = "left", 
                                             bootstrap_options = c("striped", "hover", "responsive"))
```




##**K-Means Clustering Using Principal Components**

```{r, warning = FALSE, message = FALSE}
PCA.kmeans <- kmeans(PCA$x, centers = 3)


```

Now we will attempt to build a recommendation engine based on the full user data.  The process will use a basic collaborative filtering with some experimental adjustments to predict scores.

The similarity between two users will be determined by calculating a scaled Pearson's Correlation coefficient:

$$s(u,v)=\frac{1}{\vert{D(N)\vert}^\alpha}\frac{\sum_{i\in I_{common}}(r_{u,i}-\overline{r}_{u,i})(r_{v,i}-\overline{r}_{v,i})}{\sqrt{\sum_{i\in I_{common}}(r_{u,i}-\overline{r}_{u,i})^2}\sqrt{\sum_{i\in I_{common}}(r_{v,i}-\overline{r}_{v,i})^2}}$$
Where $\vert D_{u,v}(N)\vert$ is the absolute difference in the number of ratings between user $u$ and $v$.  I include this adjustment under the suspicion that users who rate a lot of games tend form a group that rates games similarly.  The parameter $\alpha$ controls the magnitude of this effect and will be optimized for predictive power later.  For now, we will perform the analysis assuming $\alpha = 1$



```{r, message = FALSE, warning = FALSE}
getUsers <- function(df, itemID){
  quoted_item <- enquo(itemID)
  print(quoted_item)
  
  df %>% filter(!is.na(!!quoted_item))
}

similarity <- function(user1, user2, alpha = 1){
  df <- rbind(user1, user2)
  df <- df[,which((!is.na(df[1,])) & (!is.na(df[2,])))]
  
  diff <- sum(!is.na(user1)) - sum(!is.na(user2))
  
  score = 1/abs(diff)^alpha*cor(as.matrix(df)[1,1:ncol(df)], as.matrix(df)[2,1:ncol(df)])
  
  return(score)
}

user1 <- elite_sparse[1,]
user2 <- elite_sparse[10,]

similarity(user1, user2, 1)

#predict_score pseudocode(df, user, productID)
#
#1) get list of all users who have rated product given by productID
#
#VALUES NEEDED: average rating for user of interest:
#               average rating for each other user
#
#2)for each user in list of users in step 1.
#  a) compute the similarity to UOI
#     multiply by the difference of that users rating for item and and their average rating
#      sum all values
#  
#  b)compute the -absolute- similarity to UOI
#    sum all values
#
#3)compute <average rating for UOI - a)/b)>
#

predict_scores(df, users, itemID){
  users <- getUsers(df, itemID)
  
  users <- users %>% mutate(Mean = rowMeans(users)) %>%
           rowwise()
           
}

f <- data.frame(A=rnorm(10), B=rnorm(10), C=rnorm(10))

f %>% rowwise() %>% mutate(Mean = rowMeans(df))


```
Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).
