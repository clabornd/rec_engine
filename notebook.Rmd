---
title: "PCA and User-User Collaborative Filtering for Boardgame Ratings Data"
output:
  html_notebook: default
  html_document:
    code_folding: hide
---


**Credit to Matthew Borthwick for scraping and providing this data.**  
**Data Source:  boardgamegeek.com**

The data in question are user ratings for boardgames.  This notebook covers the cleaning/reshaping of the data, principal component analysis, and ratings prediction with User-User collaborative filtering.

Our tasks are as follows:

* Reshape the data such that it is in a format that is useful for our analyses
* Impute values for the NA's
* Perform PCA 
* Implement a prediction engine using user-user collaborative filtering

Lets first download the data and take a look.

```{r summary, results = 'asis'}

source("requirements.R")
ggthemr("dust")

elite <- read_csv("Data/boardgame-elite-users.csv") %>% rename(userID = `Compiled from boardgamegeek.com by Matt Borthwick`)

titles <- read_csv("Data/boardgame-titles.csv") %>% rename(gameID = `boardgamegeek.com game ID`)

test <- read_csv("Data/boardgame-users-test.csv")

test <- test %>% inner_join(elite_sparse)

elite[1:5,] %>% kable("html") %>%
                kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))

titles[1:5,] %>% kable("html") %>%
                 kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))

```

<br>

For each row, we have a user ID, a game ID, and a rating for the game by the indicated user.  We can explore the data a bit more.  Lets calculate the average and standard deviation of ratings for each game as well as how many times it was reviewed.  A plot of average rating shows somewhat of an upward trend as number of reviews increases.

<br>

```{r join_meansdplot, fig.cap = "**Scatterplots of mean rating (red) and standard deviation (blue) against number of reviews**"}
#Group by game, calculate the within gruoup average, standard deviation, and ratings count, and join with the titles dataframe.
elite <- elite %>% group_by(gameID) %>% 
              mutate(avg_rating = mean(rating), sd_rating = sd(rating), freq = n()) %>%  
              left_join(titles) 

#kable() function makes nice tables
elite[1:5,] %>% kable("html") %>%
                kable_styling(full_width = FALSE, position = "left", 
                              bootstrap_options = c("striped", "hover", "responsive"))

#Plot mean and standard deviation against review frequency
ggplot(elite, aes(x = x)) + 
  geom_point(aes(x = freq, y = sd_rating, col = "red")) + 
  geom_point(aes(x = freq, y = avg_rating, col = "blue")) +
  xlab("Number of Reviews") + ylab("Value") +
  scale_color_manual(values = c("red" = "red", "blue" = "blue"), 
                     labels = c("Average", "Standard Deviation"))
```

 

<br>

#**Data Preparation**

We ultimately want our data to contain a **single row for each user** where the columns for that user represent their rating on **each game among all games**

To get the data in this format, we lean on dplyr, and the spread() function in particular, which will convert the data into a wide format:


```{r long2wide}
elite_sparse <- elite%>%
                select(userID, gameID, rating) %>%
                spread(gameID, rating)

elite_sparse[1:5, 1:15] %>% kable("html") %>% 
                            kable_styling(full_width = FALSE, position = "left", 
                                          bootstrap_options = c("striped", "hover", "responsive"))

```

As you can see, each reviewer does not have a score for every game, and we end up with a lot of missing values even for this set of "elite" reviewers.  There are a few methods that can be used to fill the missing cells, including using the mean/median rating.  Here I will use a set of functions from the missMDA package that uses iterative methods to find likely values.  Some of the resulting data will not make sense (negative ratings, ratings higher than the maximum), however this will still provide a workable base for performing PCA.

Two steps are performed to fill the table:
1. Estimate the number of dimensions(components) to be used in PCA
2. Fill the missing cells assuming we have a number of dimensions estimated in 1.
 
```{r estimpca, cache = TRUE}
#Estimate dimensions
ncomp <- estim_ncpPCA(as.data.frame(elite_sparse[,2:ncol(elite_sparse)]), ncp.min = 0, ncp.max = 6)

#Fill the matrix
elite_sparse_filled <- imputePCA(as.data.frame(elite_sparse[,2:ncol(elite_sparse)]), 
                                 ncp = ncomp$ncp, scale = TRUE,
                                 method = "EM", row.w = NULL, coeff.ridge = 1,
                                 threshold = 1e-06, seed = NULL, nb.init = 1, maxiter = 1000)$completeObs
#Perform PCA
PCA <- prcomp(elite_sparse_filled, retx = TRUE)

#Dataframe for plotting
components <- as.data.frame(PCA$rotation) %>% mutate(gameID = as.numeric(rownames(.))) %>% left_join(titles) 


```


We graph the data in terms of pairs of principal components.  We can see how PCA chooses the component axes:  The data varies most in the direction of principal component 1, then principal component 2, and so on.  In our case, the variance explained by each component appears to stabilize after the first.

<br>

```{r plotpca, fig.cap = "**Pairs of the first 7 principal components**"}
theme <- list(geom_point(color = "black", fill = "green", pch = 21),
              theme(plot.margin=unit(c(0.25,0.25,0.25,0.25), "cm"),
                    axis.title.x = element_text(size = 8, vjust = -0.5),
                    axis.title.y = element_text(size = 8)
                    ), xlim(-60, 60), ylim(-60, 60))

grid.arrange(
ggplot(as.data.frame(PCA$x), aes(x = PC1, y=PC2)) + theme,
ggplot(as.data.frame(PCA$x), aes(x = PC2, y=PC3)) + theme,
ggplot(as.data.frame(PCA$x), aes(x = PC3, y=PC4)) + theme,
ggplot(as.data.frame(PCA$x), aes(x = PC4, y=PC5)) + theme,
ggplot(as.data.frame(PCA$x), aes(x = PC5, y=PC6)) + theme,
ggplot(as.data.frame(PCA$x), aes(x = PC6, y=PC7)) + theme,
ncol = 3
)
```

<br>

We can look at the top 10 loadings for each principal component and see what kinds of games are included to see if they describe some sort of genre or series of games.

```{r top_n, warning = FALSE, message = FALSE}

cbind(top_n(components, 10, PC1) %>% arrange(desc(PC1)) %>% select(PC1, title),
      top_n(components, 10, PC2) %>% select(PC2, title),
      top_n(components, 10, PC3) %>% select(PC3, title)) %>% kable("html") %>% 
                               kable_styling(full_width = FALSE, position = "left", 
                                             bootstrap_options = c("striped", "hover", "responsive"))
```

Looking at the descriptions for the lists of games, there does not appear to be a common theme except that the first PC loads heavily on Settlers of Catan and its expansions.  I cannot see a reason for attributing a theme to the other components aside from confirmation bias.

A quick plot of the cumulative explained variance from each component shows greatly diminshed returns after the 6th or so PC.  The variance explained by the first PC is significantly greater than the rest.

```{r explvar, warning = FALSE, message = FALSE}
explained_variance <- PCA$sdev^2/sum(PCA$sdev^2)

qplot(0:199, c(0,cumsum(explained_variance))) + xlab("Principal Component #") + ylab("Cumulative Explained Variance")
```

Aside from exploratory analysis, we can use these components for further analysis such as simple regression or K-means clustering.  For now, we will put this PCA aside and move on to building a recommendation engine from scratch.

<br>

##Recommendation Engine with User-User Collaborative Filtering.

We will now attempt to build a recommendation engine for the elite user data.  The process will use a basic User-User collaborative filtering scheme with some experimental adjustments to predict scores.

The similarity between two users will be determined by calculating a scaled Pearson's Correlation coefficient:

$$s(u,v)=\frac{1}{\vert{D_{u,v}(N)\vert}^\alpha}{\huge[}\frac{\sum_{i\in I_{common}}(r_{u,i}-\overline{r}_{u})(r_{v,i}-\overline{r}_{v})}{\sqrt{\sum_{i\in I_{common}}(r_{u,i}-\overline{r}_{u})^2}\sqrt{\sum_{i\in I_{common}}(r_{v,i}-\overline{r}_{v})^2}}+1{\huge]}$$
Where $r_{u,i}$ and $\overline{r}_{u}$ are user $u$'s rating for item $i$ and $u$'s average rating across *all* items respectively.  $I_{common}$ denotes the set of all items that $u$ and $v$ have both rated.  $\vert D_{u,v}(N)\vert$ is the absolute difference in the number of ratings between user $u$ and $v$.  I include this adjustment under the suspicion that users who rate a lot of games tend form a group that rates games similarly.  The parameter $\alpha$ controls the magnitude of this effect and will be optimized for predictive power later.  For now, we will perform the analysis assuming $\alpha = 1$

We will write three functions to do our predictions.  The first, `similarity(user1, user2)`, computes the similarity score between two users.  The second, `predict_score(test, train, itemID)` predicts a score given a user, training data, and an item ID.  The formula begins with the user-in-question's average rating, and moves up or down depending on other users rating of the game in question weighted by the similarity score:

$$p_{u,i} = \overline{r}_u+\frac{\sum_{u'\in U'}s(u,u')(r_{u',i}-\overline{r}_{u'})}{\sum_{u'\in U'}\vert s(u,u')\vert}$$
where $p_{u,i}$ is the predicted score for user $u$ and item $i$, $\overline{r}_u$ is the average score for user $u$.  $U'$ and $u'$ indicate the set of all *other* users and an individual *other* user respectively.

The last function, `test_batch(test, train)`, loops over a batch of test cases.  For each test case, it randomly selects an item with a rating and holds it out, and then attempts to predict the score for that item using `predict_score()`.  At the end of the loop a dataframe containing the predicted/true values and the item ID will be returned.

```{r functions, message = FALSE, warning = FALSE, comment = FALSE}

similarity <- function(user1, user2, alpha = 1, mode = "unweighted"){
  df <- rbind(user1, user2)
  df <- df[,which((!is.na(df[1,])) & (!is.na(df[2,])))]
  
  diff <- sum(!is.na(user1)) - sum(!is.na(user2))
  
  if(mode == "inverse power"){
    score = 1/(abs(diff)+1)^alpha*(cor(as.matrix(df)[1,], as.matrix(df)[2,]))
  }
  else if(mode == "exponential"){
    score = exp(-abs(diff)*alpha)*(cor(as.matrix(df)[1,], as.matrix(df)[2,]))
  }
  else if(mode == "linear"){
    score = alpha/(abs(diff)+1)*(cor(as.matrix(df)[1,], as.matrix(df)[2,]))
  }
  else if(mode == "unweighted"){
    score = cor(as.matrix(df)[1,], as.matrix(df)[2,])
  }
    
  return(score)
  
}

predict_score <- function(users, df_train, sim_matrix = NULL, itemID, sd = FALSE, alpha = 1, mode = "unweighted"){
  
  ##add mean and standard deviation column to users....
  users <- users %>% 
           rowwise() %>% 
           mutate(n = sum(!is.na(.))) %>% 
           ungroup() %>% 
           mutate(mean = apply(users %>% select(-n), 1, mean, na.rm = TRUE), sd = apply(users %>% select(-n), 1, sd, na.rm = TRUE))
  
  ##...And training matrix
  df_train <-  df_train %>%
               rowwise() %>% 
               mutate(n = sum(!is.na(.))) %>% 
               ungroup() %>%
               mutate(mean = apply(as.matrix(df_train %>% select(-n)), 1, mean, na.rm = TRUE), 
               sd = apply(as.matrix(df_train %>% select(-n)), 1, sd, na.rm = TRUE)) %>%
               filter(sd != 0)
  
  ##BEGIN LOOP
  preds <- 0
  
  for(i in 1:length(itemID)){
    relevant_users <- df_train %>% filter(!is.na(!!sym(itemID[i]))) 
    
    ###compare similarity to user at index 
    if(is.null(sim_matrix)){
      relevant_users <- relevant_users %>% 
                        mutate(simscore = apply(relevant_users %>% select(c(-n, -mean, -sd)), 1, similarity, user2 = users[i,], alpha = alpha, mode = mode))
      
      if(sd){
        preds[i] = users[i,]$mean + 
              users[i,]$sd*sum(relevant_users$simscore*((relevant_users %>% select(itemID[i]))-relevant_users$mean)/relevant_users$sd)/
              sum(abs(relevant_users$simscore))  
            }
      else{
        preds[i] = users[i,]$mean + 
              sum(relevant_users$simscore*((relevant_users %>% select(itemID[i]))-relevant_users$mean))/
              sum(abs(relevant_users$simscore)) 
        }
    }
    else{
      print("whoops, were here")
        ######USE PRE-CALCULATED SIMILARITY MATRIX HERE#####
        }
  }
  
  return(preds)
  
}

###################################################################
###################################################################

###################################################################
################FOR PRE SIMILARITY SCORE CALCULATIONS##############
###################################################################

#'@param df A dataframe where the rows represent users and the named columns are items.  Each element holds a numeric rating. 
#'@param alpha Smoothing parameter which determined the degree to which users with a different number of total ratings are more/less similar.  Default to 1.
#'@param mode Method for determining how similarity decays as difference in number of ratings between two users increases.  Default to unweighted(does nothing)
user_similarity_matrix <- function(usermatrix, alpha = 1, mode = "unweighted"){
  sim_matrix <- matrix(0, nrow(usermatrix), nrow(usermatrix))
  
  for(i in 1:(nrow(usermatrix)-1)){
    for(j in (i+1):nrow(usermatrix)){
      sim_matrix[i,j] <- sim_matrix[j,i] <- similarity(usermatrix[i,], usermatrix[j,], alpha = alpha, mode = mode)
      
    }
  }
  
  diag(sim_matrix) = 1
  
  return(as.data.frame(sim_matrix, row.names = rownames(usermatrix), col.names = rownames(usermatrix)))
  
}


###
#'@param df A dataframe where the rows represent users and the named columns are items.  Each element holds a numeric rating.  This should be a dataframe of the -subset- of users whose scores you want to predict.
#'@param mode Whether the values to be held out and predicted should be randomly selected or selected from a list of values.  Can be set to 'random' or 'list'.
#'@param target_list The list of items to be held out if mode is set to 'list'
#'@param return_reduced Whether or not to return the dataframe with values to predict held out.
get_targets <- function(df, mode = "random", target_vector = NULL, return_reduced = TRUE){
  
  true <- 0
  item <- NA
  
  if(mode == "random"){
    for(i in 1:nrow(df)){
      #get a random column which is not empty
        #get the name of that column
        #store these in "true" and "item" respectively
        index <- sample(which(!is.na(df[i,])), 1)
        
        true[i] <- df[i,index][[1]] 
        item[i] <- as.character(colnames(df[,index]))
        
        df[i, item[i]] <- NA
    }
    
    df = cbind.data.frame(item,true, df)
    df$item <- as.character(df$item)
    
    return(df)
  }
  
  else if(mode == "selected"){
    ###Select from a preselected list of holdout items.
    for(i in 1:nrow(df)){
      true[i] <- df[i, target_vector[i]][[1]]
      df[i, target_vector[i]] <- NA
    }
    
    return(cbind(target_vector, true, df))
  }
}

###FOR MATTS DATA, THIS SHOULD STILL WORK####
###THE SELECTED DATA WILL EXTRACTED BY AN INNER JOIN AND THEN PASSED TO THIS FUNCTION###



```

We will randomly select 20% of the elite users and use them as the test cases.  The rest will be used alongside each individual test case to calculate a predicted score.  

```{r test_nosd, message = FALSE, cache = TRUE}

indices <- sample(1:nrow(elite_sparse), floor(.1*nrow(elite_sparse)))
  
test <- elite_sparse[indices, -1]
test <- get_targets(test)
train <- elite_sparse[-indices, -1]

predict_score(test[c(-1,-2)], elite_sparse, itemID = test$item)  

preds <- test_batch(test, train)

head(preds) %>% kable("html") %>% 
                            kable_styling(full_width = FALSE, position = "left", 
                                          bootstrap_options = c("striped", "hover", "responsive"))
```

To see how our engine did, we can look at the root mean squared error of our predictions.  This is the root of the average squared difference between the predicted and true values:  $RMSE = \sqrt{\frac{1}{U_{test}}\sum_u(p_{u,i}-r_{u,i})}$

```{r RMSE, echo = TRUE}

RMSE <- sqrt(mean((preds$predicted - preds$true)^2))

RMSE

```

On average, our predictions are off by almost one and a half points, not particularly impressive.  To improve performance, we can scale the difference for each user score from their mean by the standard deviation of that users ratings:

$$p_{u,i} = \overline{r}_u+\sigma_u\frac{\sum_{u'\in U'}s(u,u')(r_{u',i}-\overline{r}_{u'})/\sigma_{u'}}{\sum_{u'\in U'}\vert s(u,u')\vert}$$

Where $\sigma_u$ and $\sigma_{u'}$ indicate the standard deviation of the scores for the user in question and an arbitrary *other* user respectively.  Lets see if our predictions improved:

```{r test_sd, cache = TRUE}

preds <- test_batch(test, train, sd = TRUE, alpha = 0.005, mode = "exponential")

RMSE <- sqrt(mean((preds$predicted - preds$true)^2))

RMSE

```

We see a slight improvement, and our engine is a little over a point off on average.  Not terrible for a basic implementation, though one problem of note is that the process takes a non-trivial amount of computing time.  In practice, this may be an issue, and is one of the main reasons why Item-Item filtering is preferred in some cases.  Next, we can try manipulating the $\alpha$ parameter to see if it improves performance.  Additionally, we can change the form of the correlation drop-off to something with linear or exponential decay.
```{r full matrix}

###HOLD OUT SCORES FIRST, THEN CALCULATE SIMILARITY###

###STORE SCORE, ITEM ID, AND REPLACE USER SCORE WITH NA####

sim_matrices <- list()
for(i in 1:nrow(elite)){
  
  
  
}


```



```{python}
import numpy as np
import feather

```
