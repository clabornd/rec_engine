---
title: "Performing PCA on game rating data"
output:
  html_document: default
  html_notebook: default
---

This notebook covers the cleaning and subsequent PCA analysis of the elite user ratings data.  

**Credit to Matthew Borthwick for scraping and providing this data.**  
**Data Source:  boardgamegeek.com**


Our tasks are as follows:

* Reshape the data such that it is in a format that PCA likes
* Impute values for the NA's
* Perform PCA
* Perform K-means clustering using the principal components

Lets first download the data and take a look.

```{r, warning = FALSE, message = FALSE, results = 'asis'}

source("requirements.R")
ggthemr("dust")

elite <- read_csv("Data/boardgame-elite-users.csv") %>% rename(userID = `Compiled from boardgamegeek.com by Matt Borthwick`)

titles <- read_csv("Data/boardgame-titles.csv") %>% rename(gameID = `boardgamegeek.com game ID`)

elite[1:5,] %>% kable("html") %>%
                kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))

titles[1:5,] %>% kable("html") %>%
                 kable_styling(full_width = FALSE, position = "left", bootstrap_options = c("striped", "hover"))

```

<br>

We can explore the data a bit more.  Lets calculate the average and standard deviation of ratings for each game as well as how many times it was reviewed.  A plot of average rating shows somewhat of an upward trend as number of reviews increases.

<br>

```{r, warning = FALSE, message = FALSE}
#Group by game, calculate the within gruoup average, standard deviation, and ratings count, and join with the titles dataframe.
elite <- elite %>% group_by(gameID) %>% 
              mutate(avg_rating = mean(rating), sd_rating = sd(rating), freq = n()) %>%  
              left_join(titles) 

#kable() function makes nice tables
elite[1:5,] %>% kable("html") %>%
                kable_styling(full_width = FALSE, position = "left", 
                              bootstrap_options = c("striped", "hover", "responsive"))

#Plot mean and standard deviation against review frequency
ggplot(elite, aes(x = x)) + 
  geom_point(aes(x = freq, y = sd_rating, col = "red")) + 
  geom_point(aes(x = freq, y = avg_rating, col = "blue")) +
  xlab("Number of Reviews") + ylab("Value") +
  scale_color_manual(values = c("red" = "red", "blue" = "blue"), 
                     labels = c("Average", "Standard Deviation"))
```

 

<br>

##**Data Preparation**

We ultimately want our data to contain a **single row for each user** where the columns for that user represent their rating on **each game among all games**

To get the data in this format, we lean on dplyr, and the spread() function in particular, which will convert the data into a wide format.  The below dplyr pipeline performs the following tasks from top to bottom:


```{r, warning = FALSE, message = FALSE}
elite_sparse <- elite%>%
                select(userID, gameID, rating) %>%
                spread(gameID, rating)

elite_sparse[1:5, 1:15] %>% kable("html") %>% 
                            kable_styling(full_width = FALSE, position = "left", 
                                          bootstrap_options = c("striped", "hover", "responsive"))

```

As you can see, each reviewer does not have a score for every game, and we end up with a lot of missing values even for this set of "elite" reviewers.  There are a few methods that can be used to fill the missing cells, including using the mean/median rating.  Here I will use a set of functions from the missMDA package that uses iterative methods to find likely values.  Some of the resulting data will not make sense (negative ratings, ratings higher than the maximum), however this will still provide a workable base for performing PCA.

Two steps are performed to fill the table:
1. Estimate the number of dimensions(components) to be used in PCA
2. Fill the missing cells assuming we have a number of dimensions estimated in 1.
 
```{r, warning = FALSE, message = FALSE}
ncomp <- estim_ncpPCA(as.data.frame(elite_sparse[,2:ncol(elite_sparse)]), ncp.min = 0, ncp.max = 6)

elite_sparse_filled <- imputePCA(as.data.frame(elite_sparse[,2:ncol(elite_sparse)]), 
                                 ncp = ncomp$ncp, scale = TRUE,
                                 method = "EM", row.w = NULL, coeff.ridge = 1,
                                 threshold = 1e-06, seed = NULL, nb.init = 1, maxiter = 1000)$completeObs

PCA <- prcomp(elite_sparse_filled, retx = TRUE)

components <- as.data.frame(PCA$rotation) %>% mutate(gameID = as.numeric(rownames(.))) %>% left_join(titles) 


```


We graph the data in terms of pairs of principal components.  We can see how PCA chooses the component axes.  The data varies most in the direction of principal component 1, then principal component 2, and so on.  However we

<br>

```{r, warning = FALSE, message = FALSE, fig.cap = "Pairs of the first 7 principal components"}
theme <- list(geom_point(color = "black", fill = "green", pch = 21),
              theme(plot.margin=unit(c(0.25,0.25,0.25,0.25), "cm"),
                    axis.title.x = element_text(size = 8, vjust = -0.5),
                    axis.title.y = element_text(size = 8)
                    ), xlim(-60, 60), ylim(-60, 60))

grid.arrange(
ggplot(as.data.frame(PCA$x), aes(x = PC1, y=PC2)) + theme,
ggplot(as.data.frame(PCA$x), aes(x = PC2, y=PC3)) + theme,
ggplot(as.data.frame(PCA$x), aes(x = PC3, y=PC4)) + theme,
ggplot(as.data.frame(PCA$x), aes(x = PC4, y=PC5)) + theme,
ggplot(as.data.frame(PCA$x), aes(x = PC5, y=PC6)) + theme,
ggplot(as.data.frame(PCA$x), aes(x = PC6, y=PC7)) + theme,
ncol = 3
)
```

<br>

We can look at the top 10 loadings for each principal component and see what kinds of games are included to see if they describe some sort of genre or series of games.

```{r, warning = FALSE, message = FALSE}

cbind(top_n(components, 10, PC1) %>% arrange(desc(PC1)) %>% select(PC1, title),
      top_n(components, 5, PC2) %>% select(PC2, title),
      top_n(components, 5, PC3) %>% select(PC3, title)) %>% kable("html") %>% 
                               kable_styling(full_width = FALSE, position = "left", 
                                             bootstrap_options = c("striped", "hover", "responsive"))
```

```{r, warning = FALSE, message = FALSE}
explained_variance <- PCA$sdev^2/sum(PCA$sdev^2)
```

Aside from exploratory analysis, we can use these componenets for further analysis such as simple regression or K-means clustering:

```{r, warning = FALSE, message = FALSE}
#PCA.kmeans <- kmeans(PCA$x[,1:6], centers = 3)


```

Now we will attempt to build a recommendation engine based on the full user data.  The process will use a basic User-User collaborative filtering scheme with some experimental adjustments to predict scores.

The similarity between two users will be determined by calculating a scaled Pearson's Correlation coefficient:

$$s(u,v)=\frac{1}{\vert{D(N)\vert}^\alpha}\frac{\sum_{i\in I_{common}}(r_{u,i}-\overline{r}_{u,i})(r_{v,i}-\overline{r}_{v,i})}{\sqrt{\sum_{i\in I_{common}}(r_{u,i}-\overline{r}_{u,i})^2}\sqrt{\sum_{i\in I_{common}}(r_{v,i}-\overline{r}_{v,i})^2}}$$
Where $\vert D_{u,v}(N)\vert$ is the absolute difference in the number of ratings between user $u$ and $v$.  I include this adjustment under the suspicion that users who rate a lot of games tend form a group that rates games similarly.  The parameter $\alpha$ controls the magnitude of this effect and will be optimized for predictive power later.  For now, we will perform the analysis assuming $\alpha = 1$

We will write three functions to do our predictions.  The first, `similarity(user1, user2)`, computes the similarity score between two users.  The second, `predict_score(test, train, itemID)` predicts a score given a user, training data, and an item ID.  The last function, `test_batch(test, train)`, loops over a batch of test cases.  For each test case, it randomly selects an item with a rating and holds it out, and then attempts to predict the score for that item using `predict_score()`.  At the end of the loop a dataframe containing the predicted/true values and the item ID will be returned.

```{r, message = FALSE, warning = FALSE, echo = FALSE, comment = FALSE}
#getUsers <- function(df, itemID){
#  quoted_item <- enquo(itemID)
#  
#  df %>% filter(!is.na(!!quoted_item))
#}

similarity <- function(user1, user2, alpha = 1){
  df <- rbind(user1, user2)
  df <- df[,which((!is.na(df[1,])) & (!is.na(df[2,])))]
  
  diff <- sum(!is.na(user1)) - sum(!is.na(user2))
  
  score = 1/(abs(diff)+1)^alpha*(cor(as.matrix(df)[1,], as.matrix(df)[2,])+1)
  
  return(score)
}

predict_score <- function(user, df_train, itemID){
  
  user <- user %>% 
          rowwise() %>% 
          mutate(n = sum(!is.na(.))) %>% 
          ungroup() %>% 
          mutate(mean = apply(user %>% select(-n), 1, mean, na.rm = TRUE), sd = apply(user %>% select(-n), 1, sd, na.rm = TRUE))

  
  relevant_users <- df_train %>% filter(!is.na(!!sym(itemID))) 
  #someone please tell me why the pipe must be broken here
  
  
  relevant_users <-  relevant_users %>%
                              rowwise() %>% 
                              mutate(n = sum(!is.na(.))) %>% 
                              ungroup() %>%
                              mutate(mean = apply(as.matrix(relevant_users %>% select(-n)), 1, mean, na.rm = TRUE), 
                                     sd = apply(as.matrix(relevant_users %>% select(-n)), 1, sd, na.rm = TRUE)) %>%
                                      filter(sd != 0)
  
  #we must do this part of the dplyr pipe seperately due to having to call the data frame within apply()   
  relevant_users <- relevant_users %>% 
    mutate(simscore = apply(relevant_users %>% select(c(-n, -mean, -sd)), 1, similarity, user))
  
  predict = user$mean + sum(relevant_users$simscore*((relevant_users %>% select(itemID))-relevant_users$mean))/sum(abs(relevant_users$simscore))
  
  return(predict)
}

test_batch <- function(test, train){  
  pred <- true <- 0
  item <- NA
  
  for(i in 1:nrow(test)){
      #get a random column which is not empty
      #get the name of that column
      #store these in "true" and "item" respectively
      index <- sample(which(!is.na(test[i,])), 1)
    
      true[i] <- test[i,index][[1]] 
      item[i] <- colnames(test[,index])
      
      foo <- test[i,]
      foo[item[i]] <- NA
      
      pred[i] <- predict_score(foo, train, item[i]) 
      
      
  }
  
  return(data.frame(predicted = pred, true = true, itemID = item))

}


```

We will randomly select 20% of the elite users and use them as the test cases.  The rest will be used alongside each individual test case to calculate a predicted score.  

```{r, warning = FALSE, message = FALSE}

indices <- sample(1:nrow(elite_sparse), floor(.2*nrow(elite_sparse)))
  
test <- elite_sparse[indices, -1]
train <- elite_sparse[-indices, -1]
  

preds <- test_batch(test, train)

head(preds) %>% kable("html") %>% 
                            kable_styling(full_width = FALSE, position = "left", 
                                          bootstrap_options = c("striped", "hover", "responsive"))
```

